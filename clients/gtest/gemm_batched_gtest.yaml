---
include: rocblas_common.yaml
include: known_bugs.yaml

Definitions:
  - &algorithm_coverage_matrix_size_range
    - { M:    64, N:   128, K:     4, lda:    64, ldb:    64, ldc:    64, ldd:    64 }
    - { M:   128, N:    64, K:     8, lda:    64, ldb:    64, ldc:    64, ldd:    64 }
    - { M:    64, N:    32, K:     8, lda:    64, ldb:    65, ldc:    66, ldd:    66 }
    - { M:    32, N:    64, K:    16, lda:    65, ldb:    66, ldc:    67, ldd:    67 }
    - { M:    25, N:    26, K:    27, lda:    28, ldb:    29, ldc:    30, ldd:    30 }

  # using stride_x,y,d to test out offsets of rocblas_internal_gemm_interface()
  - &gemm_internal_matrix_size
    - { M:  1000, N: 1001, K: 101, lda:  2002, ldb: 1003, ldc:  1004, ldd:  1004, stride_x: 4294967296, stride_y: 4294967297, stride_d: 4294967298 }
    - { M:   128, N:  128, K: 128, lda:   128, ldb:  128, ldc:   128, ldd:   128, stride_x: 4294967296, stride_y: 4294967297, stride_d: 4294967298 }
    - { M: 32768, N:  480, K:   1, lda: 32768, ldb:  480, ldc: 32768, ldd: 32768, stride_x: 4294967296, stride_y: 4294967297, stride_d: 4294967298 }

  - &small_matrix_size_range
    - { M:    -1, N:    -1, K:    -1, lda:    -1, ldb:     1, ldc:     1, ldd:     1 }
    - { M:     0, N:     8, K:     8, lda:     8, ldb:     8, ldc:     8, ldd:     8 } # m==0
    - { M:     8, N:     0, K:     8, lda:     8, ldb:     8, ldc:     8, ldd:     8 } # n==0
    - { M:     8, N:     8, K:     0, lda:     8, ldb:     8, ldc:     8, ldd:     8 } # k==0
    - { M:     8, N:     9, K:    10, lda:     8, ldb:    10, ldc:     8, ldd:     8 }
    - { M:     4, N:     3, K:     4, lda:     4, ldb:     4, ldc:     4, ldd:     4 }
    - { M:     3, N:     3, K:     3, lda:     3, ldb:     3, ldc:     3, ldd:     3 }
    - { M:     4, N:     4, K:     4, lda:     4, ldb:     4, ldc:     5, ldd:     5 }
    - { M:     8, N:     8, K:     8, lda:     8, ldb:     8, ldc:     8, ldd:     8 }
    - { M:    15, N:    15, K:    15, lda:    15, ldb:    15, ldc:    15, ldd:    15 }
    - { M:    16, N:    16, K:    16, lda:    16, ldb:    16, ldc:    16, ldd:    16 }
    - { M:    17, N:    17, K:    17, lda:    17, ldb:    17, ldc:    17, ldd:    17 }
    - { M:    31, N:    33, K:    35, lda:   101, ldb:   102, ldc:   103, ldd:   103 }
    - { M:    59, N:    61, K:    63, lda:   129, ldb:   131, ldc:   137, ldd:   137 }
    - { M:    63, N:    63, K:    63, lda:    63, ldb:    63, ldc:    63, ldd:    63 }
    - { M:    64, N:    64, K:    64, lda:    64, ldb:    64, ldc:    64, ldd:    64 }
    - { M:    65, N:    65, K:    65, lda:    65, ldb:    65, ldc:    65, ldd:    65 }
    - { M:   127, N:   127, K:   127, lda:   127, ldb:   127, ldc:   127, ldd:   127 }
    - { M:   128, N:   128, K:   128, lda:   128, ldb:   128, ldc:   128, ldd:   128 }
    - { M:   129, N:   129, K:   129, lda:   129, ldb:   129, ldc:   129, ldd:   129 }

  - &medium_matrix_size_range
    - { M:   129, N:   130, K:   131, lda:   132, ldb:   133, ldc:   134, ldd:   134 }
    - { M:   255, N:   255, K:   255, lda:   255, ldb:   255, ldc:   255, ldd:   255 }
    - { M:   256, N:   256, K:   256, lda:   256, ldb:   256, ldc:   256, ldd:   256 }
    - { M:   257, N:   257, K:   257, lda:   257, ldb:   257, ldc:   257, ldd:   257 }
    - { M:   501, N:   502, K:   103, lda:   504, ldb:   605, ldc:   506, ldd:   506 }

  - &large_matrix_size_range
    - { M:   511, N:   511, K:   511, lda:   511, ldb:   511, ldc:   511, ldd:   511 }
    - { M:   512, N:   512, K:   512, lda:   512, ldb:   512, ldc:   512, ldd:   512 }
    - { M:   513, N:   513, K:   513, lda:   513, ldb:   513, ldc:   513, ldd:   513 }
    - { M:   513, N:   514, K:   515, lda:   516, ldb:   517, ldc:   518, ldd:   518 }

  - &deepbench_vec
    - { M: 12544, N:    64, K:    64, lda: 12544, ldb:    64, ldc: 12544, ldd: 12544 }
    - { M:  3136, N:   256, K:    64, lda:  3136, ldb:    64, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:    64, K:   256, lda:  3136, ldb:   256, ldc:  3136, ldd:  3136 }
    - { M:   784, N:   128, K:   512, lda:   784, ldb:   512, ldc:   784, ldd:   784 }
    - { M:   784, N:   512, K:   128, lda:   784, ldb:   128, ldc:   784, ldd:   784 }
    - { M:   784, N:    64, K:   192, lda:   784, ldb:   192, ldc:   784, ldd:   784 }
    - { M:   196, N:  1024, K:   256, lda:   196, ldb:  1024, ldc:   196, ldd:   196 }
    - { M:   196, N:   256, K:  1024, lda:   196, ldb:   256, ldc:   196, ldd:   196 }
    - { M:   196, N:   256, K:   256, lda:   196, ldb:   256, ldc:   196, ldd:   196 }
    - { M:   196, N:   512, K:   192, lda:   196, ldb:   512, ldc:   196, ldd:   196 }
    - { M:  3136, N:   256, K:    64, lda:  3136, ldb:   256, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:    64, K:   256, lda:  3136, ldb:    64, ldc:  3136, ldd:  3136 }
    - { M:    49, N:  2048, K:   512, lda:    49, ldb:  2048, ldc:    49, ldd:    49 }
    - { M:    49, N:   512, K:  2048, lda:    49, ldb:   512, ldc:    49, ldd:    49 }
    - { M:    49, N:   512, K:   512, lda:    49, ldb:   512, ldc:    49, ldd:    49 }
    - { M:    49, N:   832, K:   256, lda:    49, ldb:   832, ldc:    49, ldd:    49 }
    - { M:   784, N:   128, K:   512, lda:   784, ldb:   128, ldc:   784, ldd:   784 }
    - { M:   784, N:   192, K:    64, lda:   784, ldb:   192, ldc:   784, ldd:   784 }
    - { M:   784, N:   512, K:   128, lda:   784, ldb:   512, ldc:   784, ldd:   784 }

  - &conv_resnet50_fwd
    - { M:  3025, N:    64, K:   256, lda:  3025, ldb:   256, ldc:  3025, ldd:  3025 }
    - { M:  3025, N:   256, K:    64, lda:  3025, ldb:    64, ldc:  3025, ldd:  3025 }

  - &conv_resnet50_bwddata
    - { M:  3025, N:   256, K:    64, lda:  3025, ldb:   256, ldc:  3025, ldd:  3025 }
    - { M:  3025, N:    64, K:   256, lda:  3025, ldb:    64, ldc:  3025, ldd:  3025 }
    - { M:  3025, N:    64, K:    64, lda:  3025, ldb:    64, ldc:  3025, ldd:  3025 }
    - { M:  3136, N:    64, K:    64, lda:  3136, ldb:    64, ldc:  3136, ldd:  3136 }

  - &conv_inception4_fwd
    - { M:  1225, N:   192, K:   384, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:  1225, N:    64, K:   384, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:  1225, N:    96, K:   384, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:   289, N:   128, K:  1024, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:   192, K:  1024, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:   256, K:  1024, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:   384, K:  1024, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:  5329, N:    64, K:   160, lda:  5329, ldb:   160, ldc:  5329, ldd:  5329 }

  - &conv_inception4_bwddata
    - { M:  1225, N:   384, K:   192, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:  1225, N:   384, K:    64, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:  1225, N:   384, K:    96, lda:  1225, ldb:   384, ldc:  1225, ldd:  1225 }
    - { M:   289, N:  1024, K:   128, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:  1024, K:   192, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:  1024, K:   256, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:   289, N:  1024, K:   384, lda:   289, ldb:  1024, ldc:   289, ldd:   289 }
    - { M:  5329, N:   160, K:    64, lda:  5329, ldb:   160, ldc:  5329, ldd:  5329 }
    - { M:    64, N:  1536, K:   256, lda:    64, ldb:  1536, ldc:    64, ldd:    64 }
    - { M:    64, N:  1536, K:   384, lda:    64, ldb:  1536, ldc:    64, ldd:    64 }

  - &conv_ctest_bwddata
    - { M:   121, N:  2048, K:     1, lda:   121, ldb:  2048, ldc:   121, ldd:   121 }
    - { M: 12544, N:    64, K:     1, lda: 12544, ldb:    64, ldc: 12544, ldd: 12544 }
    - { M:   144, N:  1024, K:     1, lda:   144, ldb:  1024, ldc:   144, ldd:   144 }
    - { M:   144, N:   256, K:     1, lda:   144, ldb:   256, ldc:   144, ldd:   144 }
    - { M:   144, N:   512, K:     1, lda:   144, ldb:   512, ldc:   144, ldd:   144 }
    - { M:   169, N:   256, K:     1, lda:   169, ldb:   256, ldc:   169, ldd:   169 }
    - { M:    16, N:   512, K:     1, lda:    16, ldb:   512, ldc:    16, ldd:    16 }
    - { M:    16, N:   528, K:     1, lda:    16, ldb:   528, ldc:    16, ldd:    16 }
    - { M:    16, N:   576, K:     1, lda:    16, ldb:   576, ldc:    16, ldd:    16 }
    - { M:    16, N:   608, K:     1, lda:    16, ldb:   608, ldc:    16, ldd:    16 }
    - { M:   196, N:   128, K:     1, lda:   196, ldb:   128, ldc:   196, ldd:   196 }
    - { M:   196, N:   192, K:     1, lda:   196, ldb:   192, ldc:   196, ldd:   196 }
    - { M:   196, N:   256, K:     1, lda:   196, ldb:   256, ldc:   196, ldd:   196 }
    - { M:   196, N:   480, K:     1, lda:   196, ldb:   480, ldc:   196, ldd:   196 }
    - { M:   196, N:   512, K:     1, lda:   196, ldb:   512, ldc:   196, ldd:   196 }
    - { M:   196, N:   528, K:     1, lda:   196, ldb:   528, ldc:   196, ldd:   196 }
    - { M:   196, N:   576, K:     1, lda:   196, ldb:   576, ldc:   196, ldd:   196 }
    - { M:   196, N:   608, K:     1, lda:   196, ldb:   608, ldc:   196, ldd:   196 }
    - { M:   196, N:    64, K:     1, lda:   196, ldb:    64, ldc:   196, ldd:   196 }
    - { M:  3136, N:   128, K:     1, lda:  3136, ldb:   128, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:   256, K:     1, lda:  3136, ldb:   256, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:    64, K:     1, lda:  3136, ldb:    64, ldc:  3136, ldd:  3136 }
    - { M: 32768, N:   480, K:     1, lda: 32768, ldb:   480, ldc: 32768, ldd: 32768 }
    - { M:    49, N:  1024, K:     1, lda:    49, ldb:  1024, ldc:    49, ldd:    49 }
    - { M:    49, N:  1056, K:     1, lda:    49, ldb:  1056, ldc:    49, ldd:    49 }
    - { M:    49, N:   192, K:     1, lda:    49, ldb:   192, ldc:    49, ldd:    49 }
    - { M:    49, N:   512, K:     1, lda:    49, ldb:   512, ldc:    49, ldd:    49 }
    - { M:    49, N:   832, K:     1, lda:    49, ldb:   832, ldc:    49, ldd:    49 }
    - { M:   729, N:    64, K:     1, lda:   729, ldb:    64, ldc:   729, ldd:   729 }
    - { M:   784, N:   128, K:     1, lda:   784, ldb:   128, ldc:   784, ldd:   784 }
    - { M:   784, N:   192, K:     1, lda:   784, ldb:   192, ldc:   784, ldd:   784 }
    - { M:   784, N:   256, K:     1, lda:   784, ldb:   256, ldc:   784, ldd:   784 }
    - { M:   784, N:   320, K:     1, lda:   784, ldb:   320, ldc:   784, ldd:   784 }
    - { M:   784, N:   512, K:     1, lda:   784, ldb:   512, ldc:   784, ldd:   784 }
    - { M:   784, N:    64, K:     1, lda:   784, ldb:    64, ldc:   784, ldd:   784 }
    - { M:  8192, N:   480, K:     1, lda:  8192, ldb:   480, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:   512, K:     1, lda:  8192, ldb:   512, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:   528, K:     1, lda:  8192, ldb:   528, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:   832, K:     1, lda:  8192, ldb:   832, ldc:  8192, ldd:  8192 }

  - &conv_ctest_fwd
    - { M: 12544, N:     1, K:    64, lda: 12544, ldb:    64, ldc: 12544, ldd: 12544 }
    - { M:  3136, N:     1, K:   128, lda:  3136, ldb:   128, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:     1, K:   256, lda:  3136, ldb:   256, ldc:  3136, ldd:  3136 }
    - { M:  3136, N:     1, K:    64, lda:  3136, ldb:    64, ldc:  3136, ldd:  3136 }
    - { M: 32768, N:     1, K:   480, lda: 32768, ldb:   480, ldc: 32768, ldd: 32768 }
    - { M:   729, N:     1, K:    64, lda:   729, ldb:    64, ldc:   729, ldd:   729 }
    - { M:   784, N:     1, K:   128, lda:   784, ldb:   128, ldc:   784, ldd:   784 }
    - { M:   784, N:     1, K:   192, lda:   784, ldb:   192, ldc:   784, ldd:   784 }
    - { M:   784, N:     1, K:   256, lda:   784, ldb:   256, ldc:   784, ldd:   784 }
    - { M:   784, N:     1, K:   320, lda:   784, ldb:   320, ldc:   784, ldd:   784 }
    - { M:   784, N:     1, K:   512, lda:   784, ldb:   512, ldc:   784, ldd:   784 }
    - { M:   784, N:     1, K:    64, lda:   784, ldb:    64, ldc:   784, ldd:   784 }
    - { M:  8192, N:     1, K:   480, lda:  8192, ldb:   480, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:     1, K:   512, lda:  8192, ldb:   512, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:     1, K:   528, lda:  8192, ldb:   528, ldc:  8192, ldd:  8192 }
    - { M:  8192, N:     1, K:   832, lda:  8192, ldb:   832, ldc:  8192, ldd:  8192 }

  - &transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: C, transB: N }
    - { transA: T, transB: C }

  - &algorithm_coverage_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }

  - &algorithm_coverage_transA_transB_conj_range
    - { transA: N, transB: C }
    - { transA: C, transB: N }
    - { transA: T, transB: C }
    - { transA: C, transB: T }
    - { transA: C, transB: C }


  - &algorithm_coverage_alpha_beta_range
    - { alpha:  1.0, beta:  1.0 }
    - { alpha:  1.0, beta: -1.0 }
    - { alpha:  1.0, beta:  0.0 }
    - { alpha: -1.0, beta:  0.0 }
    - { alpha:  2.0, beta:  0.0 }
    - { alpha:  0.0, beta:  2.0 }
    - { alpha:  2.0, beta:  3.0 }

  - &alpha_beta_range
    - { alpha:  1.0, beta:  0.0 }
    - { alpha: -2.0, beta: -3.0 }
    - { alpha:  0.0, beta:  1.0 }

  - &complex_alpha_beta_range
    - { alpha:  1, beta:  3, alphai:  3, betai:  1 }
    - { alpha: -1, beta: -3, alphai:  3, betai:  1 }

  - &alpha_beta_range_small
    - { alpha: 2, alphai: 2, beta: -1.0, betai: 2.0 }


Tests:
- name: gemm_batched_bad_arg
  category: quick
  function:
    - gemm_batched_bad_arg: *half_single_double_complex_real_precisions
  transA: N
  transB: N
  api: [ FORTRAN, FORTRAN_64 ]

- name: gemm_xx_batched_bad_arg
  category: quick
  function:
    - gemm_batched_ex_bad_arg: *real_precisions
    - gemm_batched_ex_bad_arg: *complex_precisions
    - gemm_batched_ex3_bad_arg: *float8_float8_float8
  transA: N
  transB: N
  api: [ C, FORTRAN ]

- name: gemm_batched_algorithm_complex_coverage
  category: pre_checkin
  function:
    - gemm_batched: *single_double_precisions_complex
  matrix_size: *algorithm_coverage_matrix_size_range
  pointer_mode_device: false
  transA_transB: *algorithm_coverage_transA_transB_range
  alpha_beta: *algorithm_coverage_alpha_beta_range
  batch_count: [ 1, 3 ]
  api: C

- name: gemm_batched_algorithm_conj_coverage
  category: pre_checkin
  function:
    - gemm_batched: *single_double_precisions_complex
  matrix_size: *algorithm_coverage_matrix_size_range
  pointer_mode_host: false
  transA_transB: *algorithm_coverage_transA_transB_conj_range
  alpha_beta: *algorithm_coverage_alpha_beta_range
  batch_count: [ 1, 3 ]
  api: C

- name: gemm_batched_algorithm_real_coverage
  category: pre_checkin
  function:
    - gemm_batched: *single_double_precisions
  matrix_size: *algorithm_coverage_matrix_size_range
  pointer_mode_device: false
  transA_transB: *algorithm_coverage_transA_transB_range
  alpha_beta: *algorithm_coverage_alpha_beta_range
  batch_count: [ 1, 3 ]
  api: C

# Tests confirm no NaN propagation when alpha = 0, 2 and beta = 0. Value .NaN is converted into zero
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *single_precision, batch_count: 1,
   function: gemm_batched, transA: N, transB: N, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *double_precision, batch_count: 2,
   function: gemm_batched, transA: N, transB: T, M: 128, N:  64, K: 256, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *single_precision_complex, batch_count: 3,
   function: gemm_batched, transA: T, transB: N, M:  64, N: 256, K: 128, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *double_precision_complex, batch_count: 4,
   function: gemm_batched, transA: T, transB: T, M: 128, N:  64, K: 256, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }

- {name: alpha_beta_zero_NaN, category: pre_checkin, function: gemm_batched_ex, batch_count: 5,
   a_type: f16_r, b_type: f16_r, c_type: f16_r, d_type: f16_r, compute_type: f32_r,
   transA: N, transB: N, M: 255, N: 127, K:  63, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, function: gemm_batched_ex, batch_count: 6,
   a_type: bf16_r, b_type: bf16_r, c_type: bf16_r, d_type: bf16_r, compute_type: f32_r,
   transA: N, transB: T, M:  62, N: 126, K: 254, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, function: gemm_batched_ex, batch_count: 7,
   a_type: f32_r, b_type: f32_r, c_type: f32_r, d_type: f32_r, compute_type: f32_r,
   transA: T, transB: N, M: 253, N:  61, K: 125, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }
- {name: alpha_beta_zero_NaN, category: pre_checkin, function: gemm_batched_ex, batch_count: 8,
   a_type: f64_r, b_type: f64_r, c_type: f64_r, d_type: f64_r, compute_type: f64_r,
   transA: T, transB: T, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }


# Split *real_precisions into *int8 and *nonint8_real_precisions. TODO: merge nonint8 and int8 test groups

- name: gemm_batched_fortran
  category: quick
  function:
    - gemm_batched: *half_single_double_precisions
    - gemm_batched: *single_double_precisions_complex
    - gemm_batched_ex: *nonint8_real_precisions
    - gemm_batched_ex: *single_double_precisions_complex
  matrix_size: *small_matrix_size_range
  pointer_mode_device: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range_small
  batch_count: [ -1, 0, 3 ]
  api: FORTRAN

- name: gemm_batched_int8_fortran
  category: quick
  function:
    - gemm_batched_ex: *int8_precision
  matrix_size: *small_matrix_size_range
  pointer_mode_host: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range_small
  batch_count: [ -1, 0, 3 ]
  api: FORTRAN

# general batched gemm passes offset into tensile, strided_batched
# and non-batched just offset pointer beforehand
- name: gemm_batched_internal
  category: quick
  function:
    - gemm_batched: *single_precision
  matrix_size: *gemm_internal_matrix_size
  pointer_mode_device: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]
  api: INTERNAL

- name: gemm_batched_small
  category: quick
  function:
    - gemm_batched: *half_single_double_precisions
    - gemm_batched_ex: *nonint8_real_precisions
  matrix_size: *small_matrix_size_range
  pointer_mode_host: false
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_small_int8
  category: quick
  function:
    - gemm_batched_ex: *int8_precision
  matrix_size: *small_matrix_size_range
  pointer_mode_device: false
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  batch_count: [ -1, 0, 1, 3 ]

- name: F8SS_family_batched
  category: quick
  function: gemm_batched_ex3
  precision: *F8SS_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: F8HS_family_batched
  category: quick
  function: gemm_batched_ex3
  precision: *F8HS_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: F8F8S_family_batched
  category: quick
  function: gemm_batched_ex3
  precision: *F8F8S_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  # flags: [0 , stochastic_rounding]
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage1_unit_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 224, ldc: 320, ldd: 320 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'
  api: [ C, FORTRAN ]

- name: stage1_small_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  M: [ 1, 2, 3, 4]
  N: [ 1, 2, 3, 4]
  K: [ 1, 2, 3, 4, 16, 32]
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage1_unit_SR_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  flags: [stochastic_rounding]
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage1_small_SR_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  M: [ 1, 2, 3, 4]
  N: [ 1, 2, 3, 4]
  K: [ 1, 2, 3, 4, 16, 32]
  unit_check: 1
  res_check: 0
  norm_check: 0
  flags: [stochastic_rounding]
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage1_res_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 0
  res_check: 1
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage1_res_SR_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage1_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 0
  res_check: 1
  norm_check: 0
  flags: [stochastic_rounding]
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage2_unit_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage2_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'
  api: [ C, FORTRAN ]

- name: stage2_small_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage2_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  M: [ 1, 2, 3, 4]
  N: [ 1, 2, 3, 4]
  K: [ 1, 2, 3, 4, 16, 32]
  unit_check: 1
  res_check: 0
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: stage2_res_batched
  category: quick
  function: gemm_batched_ex3
  precision: *stage2_family
  transA: [ N, T ]
  transB: [ N, T ]
  alpha_beta: *alpha_beta_range
  matrix_size:
    - { M:  32, N:  32, K:  32 }
    - { M:  32, N:  64, K:  96 }
    - { M:  32, N:  64, K:  96, lda: 128, ldb: 128, ldc: 128, ldd: 128 }
    - { M:  111, N:  777, K:  111 }
    - { M:  555, N:  777, K:  111 }
  unit_check: 0
  res_check: 1
  norm_check: 0
  initialization: rand_int_zero_one
  batch_count: [ -1, 0, 1, 3 ]
  gpu_arch: '94?'

- name: gemm_batched_small_complex
  category: quick
  function:
    gemm_batched: *single_double_precisions_complex
    gemm_batched_ex: *single_double_precisions_complex
  matrix_size: *small_matrix_size_range
  pointer_mode_host: false
  alpha_beta: *complex_alpha_beta_range
  transA_transB: *transA_transB_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_medium
  category: pre_checkin
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *medium_matrix_size_range
  pointer_mode_device: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_medium_complex
  category: pre_checkin
  function:
    gemm_batched: *single_double_precisions_complex
    gemm_batched_ex: *single_double_precisions_complex
  matrix_size: *medium_matrix_size_range
  pointer_mode_host: false
  transA_transB: *transA_transB_range
  alpha_beta: *complex_alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_large
  category: pre_checkin
  function:
    - gemm_batched: *half_single_double_precisions
    - gemm_batched_ex: *half_single_double_precisions
  matrix_size: *large_matrix_size_range
  pointer_mode_device: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_large_complex
  category: pre_checkin
  function:
    - gemm_batched: *single_double_precisions_complex
    - gemm_batched_ex: *single_double_precisions_complex
  matrix_size: *large_matrix_size_range
  pointer_mode_host: false
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemm_batched_deepbench
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *deepbench_vec
  pointer_mode_device: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: T
  batch_count: [ 2 ]

- name: gemm_batched_conv_resnet50_fwd
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_resnet50_fwd
  pointer_mode_host: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: N
  batch_count: 4

- name: gemm_batched_conv_resnet50_bwddata
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_resnet50_bwddata
  pointer_mode_device: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: T
  batch_count: 4

- name: gemm_batched_conv_inception4_fwd
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_inception4_fwd
  pointer_mode_host: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: N
  batch_count: 4

- name: gemm_batched_conv_inception4_bwddata
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_inception4_bwddata
  pointer_mode_device: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: T
  batch_count: 4

- name: gemm_batched_conv_ctest_bwddata
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_ctest_bwddata
  pointer_mode_host: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: T
  batch_count: 1

- name: gemm_batched_conv_ctest_fwd
  category: nightly
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size: *conv_ctest_fwd
  pointer_mode_device: false
  alpha: 1.0
  beta: 0.0
  transA: N
  transB: N
  batch_count: 1

- name: gemm_batched_ex_hpa_fp16
  category: quick
  function:
    gemm_batched_ex: *hpa_half_precision
  M: 128
  N: 128
  K: 1000
  pointer_mode_host: false
  transA: N
  transB: N
  alpha: 0.001
  beta: 1
  batch_count: [ 1, 5, 10, 12 ]

- name: gemm_batched_zerok
  category: quick
  function:
    gemm_batched: *half_single_double_precisions
    gemm_batched: *complex_precisions
    gemm_batched_ex: *real_precisions
    gemm_batched_ex: *complex_precisions
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  K: 0
  matrix_size:
    - { M:  1,   N:  2 }
    - { M:  3,   N:  5 }
    - { M:  512, N:  100 }
    - { M:  63,  N:  512 }
  pointer_mode_device: false
  batch_count: 1

- name: gemm_batched_graph_test
  category: pre_checkin
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched_ex: *half_single_precisions
  matrix_size:
      - { M:  512, N:  100 }
  pointer_mode_device: false
  transA: N
  transB: N
  alpha_beta: *alpha_beta_range
  batch_count: [ 3 ]
  graph_test: true

- name: gemm_batched_64_API
  category: stress
  function:
    - gemm_batched: *half_precision
  matrix_size:
    - { M: 2, N: 2, K: 2, lda: 2, ldb:    2, ldc:    2 }
  pointer_mode_device: false
  transA: [ T ]
  transB: [ N ]
  batch_count: *c_grid_yz_require_passes
  alpha: 1
  beta: 1
  api: [ C_64 ]
  os_flags: LINUX

- name: gemm_batched_repeatability_check
  category: stress
  function:
    - gemm_batched: *half_single_precisions
    - gemm_batched: *complex_precisions
    - gemm_batched_ex: *half_single_double_precisions
  matrix_size: *deepbench_vec
  initialization: hpl
  transA: [N , T]
  transB: [N, T]
  alpha_beta: *alpha_beta_range
  atomics_mode: 0
  iters: 5
  batch_count: [ 2 ]
  pointer_mode_host: false
  repeatability_check: true
...
