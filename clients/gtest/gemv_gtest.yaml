---
include: rocblas_common.yaml
include: known_bugs.yaml

Definitions:
  - &small_matrix_size_range
    - { M:    10, N:    10, lda:    2, stride_a:        1 }
    - { M:   100, N:   200, lda:  200, stride_a:    40000 }

  - &sm_mn_matrix_size_range
    # m && n <= 32 && batch_count >= 256 (gfx90a only) watch for overlap with qmcpack
    - { M:    3 , N:    32, lda:    3, stride_a:       96 }
    - { M:    24, N:    24, lda:   24, stride_a:     1024 }
    - { M:    32, N:    11, lda:   32, stride_a:     1024 }

  - &qmcpack_matrix_size_range
    # m <= 64 && batch_count > 8 (transposes only), N >= 33 to avoid sm_mn kernel
    - { M:    2 , N:    33, lda:    2, stride_a:       66 }
    - { M:    32, N:    34, lda:   32, stride_a:     1088 }
    - { M:    15, N:    35, lda:   16, stride_a:     1024 }
    - { M:    32, N:  1536, lda:   32, stride_a:    49152 }

  - &skinny_n_matrix_size_range
    # n <= 128 && m >= 2048 * n
    - { M:  32000, N:  11, lda:    32000, stride_a:   352000 }
    - { M:  81920, N:  40, lda:    81920, stride_a:  3276800 }
    - { M:  20480, N:   7, lda:    20480, stride_a:   143360 }
    - { M: 131071, N:  63, lda:   131071, stride_a:  8257473 }

  - &double_buffered_loads_size_range
    # (n <= 3000 && n >= m) || (n %128 == 0 && m==n)
    - { M:  256, N:  256, lda:  256, stride_a:   65536 }
    - { M: 1024, N: 2000, lda: 1024, stride_a: 2048000 }
    - { M:   55, N:   77, lda:   55, stride_a:    4235 }
    - { M: 1023, N: 6144, lda: 1023, stride_a: 6285312 } # test for gfx90a

  - &all_algo_matrix_size_range
    - { M:  4096, N:    2, lda:  4096, stride_a:     8192 } # skinny n
    - { M:    32, N:    32, lda:   32, stride_a:     1024 }
    - { M:   100, N:   200, lda:  200, stride_a:    40000 }

  - &special_case_range
    # Quick return
    - { M:    0, N:    1, lda:    1, incx: 1, incy: 1, batch_count: 1 }
    - { M:    1, N:    0, lda:    1, incx: 1, incy: 1, batch_count: 1 }
    - { M:    1, N:    1, lda:    1, incx: 1, incy: 1, batch_count: 0 }

    # invalid_arg checks
    - { M:   -1, N:    0, lda:    1, incx: 1, incy: 1, batch_count:  0 }
    - { M:    0, N:   -1, lda:    1, incx: 1, incy: 1, batch_count:  0 }
    - { M:    5, N:    5, lda:    4, incx: 1, incy: 1, batch_count:  0 }
    - { M:    0, N:    0, lda:    0, incx: 1, incy: 1, batch_count:  0 }
    - { M:    0, N:    0, lda:    1, incx: 0, incy: 1, batch_count:  0 }
    - { M:    0, N:    0, lda:    1, incx: 1, incy: 0, batch_count:  0 }
    - { M:    0, N:    0, lda:    1, incx: 1, incy: 1, batch_count: -1 }

  - &medium_matrix_size_range
    - { M:   300, N:   400, lda:  400, stride_a:   160000 }
    - { M:   600, N:   500, lda:  601, stride_a:   301000 }

  - &large_matrix_size_range
    - { M:  1000, N:  1000,  lda: 1000,  stride_a:  1000000 }
    - { M:  2000, N:  2000,  lda: 2000,  stride_a:  4000000 }
    - { M:  4011, N:  4011,  lda: 4011,  stride_a: 16088200 }
    - { M:  8000, N:  8000,  lda: 8000,  stride_a: 64000000 }

  - &very_large_matrix_size_range
    - { M:  16010, N: 16010, lda: 16010, stride_a: 256320100 }
    - { M:  25020, N: 25020, lda: 25020, stride_a: 625000400 }

  - &incx_incy_range
    - { incx:   2, incy:   1, stride_scale: 1   }
    - { incx:  -1, incy:   2, stride_scale: 1   }
    - { incx:   1, incy:   1, stride_scale: 1   }
    - { incx:  -1, incy:   3, stride_scale: 1.5 }
    - { incx:   3, incy:  -1, stride_scale: 2   }
    - { incx:  10, incy: 100, stride_scale: 1   }

  - &incx_incy_range_small
    - { incx: 2, incy: 2, stride_scale: 1 }

  - &incx_incy_unity
    - { incx: 1, incy: 1, stride_scale: 1 }

  - &alpha_beta_range
    - { alpha:  2.0, beta:  0.0, alphai: 1.5, betai: 0.5 }
    - { alpha: -1.0, beta: -1.0, alphai: 0.5, betai: 1.5 }
    - { alpha:  2.0, beta:  1.0, alphai: -1.5, betai: 0.5 }
    - { alpha:  0.0, beta:  1.0, alphai: -0.5, betai: 0 }

  - &alpha_beta_range_small
    - { alpha: 2.0, beta: 2.0, alphai: 1.5, betai: -1.5 }

  - &alpha_beta_range_size_t
    - { alpha: 2.0, beta: 2.0, alphai: 1.5, betai: -1.5 }
    - { alpha:   0, beta:   2, alphai:   0, betai: -1.5 }
    - { alpha: 2.0, beta:   0, alphai: 1.5, betai:    0 }
    - { alpha:   0, beta:   0, alphai:   0, betai:    0 }

  # Non-transpose: batched gfx90a
  # Not a feasable test with current memory contraints
  # - &size_t_large_batch
  #   - { M: 2, N: 2, lda: 1073741825, incx: *c_pos_x2_overflow_int32, incy: 1, batch_count: 256 }
  #   - { M: 2, N: 2, lda: 2, incx: *c_pos_x2_overflow_int32, incy: 1, batch_count: 256 }
  #   - { M: 2, N: 2, lda: 2, incx: *c_neg_x2_overflow_int32, incy: 1, batch_count: 256 }
  #   - { M: 2, N: 2, lda: 2, incx: 1, incy: *c_pos_x2_overflow_int32, batch_count: 256 }
  #   - { M: 2, N: 2, lda: 2, incx: 1, incy: *c_neg_x2_overflow_int32, batch_count: 256 }

  # Non-transpose: non-batched 90a tests
  - &size_t_non_transpose_90a
    # skinny
    - { M: 4096, N: 2, lda: 524416, incx: 1, incy: 1 }
    - { M: 4096, N: 2, lda:   4096, incx: 524416, incy: 1 }
    - { M: 4096, N: 2, lda:   4096, incx: -524416, incy: 1 }
    - { M: 4096, N: 2, lda:   4096, incx: 1, incy:  524416 }
    - { M: 4096, N: 2, lda:   4096, incx: 1, incy: -524416 }
    # gfx90a double buffered loads, w/ atomics
    - { M: 128, N: 128, lda: 16909322, incx: 1, incy: 1 }
    - { M: 128, N: 128, lda: 128, incx: 16909322, incy: 1 }
    - { M: 128, N: 128, lda: 128, incx: -16909322, incy: 1 }
    - { M: 128, N: 128, lda: 128, incx: 1, incy: 16909322 }
    - { M: 128, N: 128, lda: 128, incx: 1, incy: -16909322 }
    # general 90a
    - { M: 3, N: 3, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_pos_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_neg_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_pos_x2_overflow_int32 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_neg_x2_overflow_int32 }

  # Non-transpose: gfx908 optimization, m < 15000, n < 15000
  - &size_t_908
    - { M: 3, N: 3, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_pos_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_neg_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_pos_x2_overflow_int32 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_neg_x2_overflow_int32 }

  # half/bfloat tests: using generic precision kernels currently
  - &size_t_fp16
    - { M: 3, N: 3, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1, batch_count: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_pos_x2_overflow_int32, incy: 1, batch_count: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_neg_x2_overflow_int32, incy: 1, batch_count: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_pos_x2_overflow_int32, batch_count: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_neg_x2_overflow_int32, batch_count: 1 }

  # Transpose/Conj: batched gfx90a
  # Not a feasable test with current memory contraints
  # - &size_t_batch10
  #   - { M: 2, N: 2, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1, batch_count: 9 }
  #   - { M: 2, N: 2, lda: 2, incx: *c_pos_x2_overflow_int32, incy: 1, batch_count: 9 }
  #   - { M: 2, N: 2, lda: 2, incx: *c_neg_x2_overflow_int32, incy: 1, batch_count: 9 }
  #   - { M: 2, N: 2, lda: 2, incx: 1, incy: *c_pos_x2_overflow_int32, batch_count: 9 }
  #   - { M: 2, N: 2, lda: 2, incx: 1, incy: *c_neg_x2_overflow_int32, batch_count: 9 }

  # Transpose/Conj: non-batched 90a tests
  - &size_t_transpose_90a
    # skinny
    - { M: 8192, N: 4, lda: 262177, incx: 1, incy: 1 }
    - { M: 8192, N: 4, lda: 8192, incx: 262177, incy: 1 }
    - { M: 8192, N: 4, lda: 8192, incx: -262177, incy: 1 }
    - { M: 8192, N: 4, lda: 8192, incx: 1, incy: *c_pos_x2_overflow_int32 }
    - { M: 8192, N: 4, lda: 8192, incx: 1, incy: *c_neg_x2_overflow_int32 }
    # general hardware shared-mem reduction
    - { M: 3, N: 3, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_pos_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_neg_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_pos_x2_overflow_int32 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_neg_x2_overflow_int32 }
    #general case (m & n have to be reasonably large)
    - { M: 4193, N: 4194, lda: 512282, incx: 1, incy: 1 }
    - { M: 4193, N: 4194, lda: 4193, incx: 512282, incy: 1 }
    - { M: 4193, N: 4194, lda: 4193, incx: -512282, incy: 1 }
    - { M: 4193, N: 4194, lda: 4193, incx: 1, incy: 512282 }
    - { M: 4193, N: 4194, lda: 4193, incx: 1, incy: -512282 }

  # Conj: general case m > 6000, n > 6000 non-float
  - &size_t_general_c_90a
    - { M: 6001, N: 6001, lda: 357914, incx: 1, incy: 1 }
    - { M: 6001, N: 6001, lda: 6001, incx: 357914, incy: 1 }
    - { M: 6001, N: 6001, lda: 6001, incx: -357914, incy: 1 }
    - { M: 6001, N: 6001, lda: 6001, incx: 1, incy: 357914 }
    - { M: 6001, N: 6001, lda: 6001, incx: 1, incy: -357914 }

  # Transpose/Conj: 908 double buffered loads, atomics allowed
  - &size_t_db_loads_t_908
    - { M: 10496, N: 10496, lda: 204620, incx: 1, incy: 1 }
    - { M: 10496, N: 10496, lda: 10496, incx: 204620, incy: 1 }
    - { M: 10496, N: 10496, lda: 10496, incx: 204620, incy: 1 }
    - { M: 10496, N: 10496, lda: 10496, incx: 1, incy: 204620 }
    - { M: 10496, N: 10496, lda: 10496, incx: 1, incy: 204620 }

  # Transpose: gfx10/11 warp-reduction
  - &size_t_gfx10_wr
    - { M: 3, N: 3, lda: *c_pos_x2_overflow_int32, incx: 1, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_pos_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: *c_neg_x2_overflow_int32, incy: 1 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_pos_x2_overflow_int32 }
    - { M: 3, N: 3, lda: 3, incx: 1, incy: *c_neg_x2_overflow_int32 }

Tests:
# Regular gemv
- name: gemv_bad_arg
  category: pre_checkin
  function: gemv_bad_arg
  precision: *single_double_precisions
  transA: N
  api: [ C, FORTRAN ]

- name: gemv_arg_check
  category: quick
  function: gemv
  precision: *single_double_precisions
  transA: N

- name: gemv_NaN
  category: pre_checkin
  function: gemv
  precision: *single_double_precisions
  transA: [ N, T ]
  matrix_size: *all_algo_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha: [ 1.0, .NaN ]  # NaN is converted to 0.0 in test code
  beta: [ 0.5, 1.0, .NaN ]

- name: gemv_fortran
  category: quick
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  api: FORTRAN

- name: gemv_small
  category: quick
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *small_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range

- name: gemv_medium
  category: pre_checkin
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *medium_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range

- name: gemv_double_buffered
  category: pre_checkin
  function: gemv
  precision: *single_double_precisions
  transA: [ N ]
  matrix_size: *double_buffered_loads_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  atomics_mode: [0, 1]

- name: gemv_medium_HMM
  category: HMM
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N ]
  matrix_size: *medium_matrix_size_range
  incx: -1
  incy: 2
  alpha: 1
  beta: 1

- name: gemv_medium_multi_gpu
  category: multi_gpu
  function: gemv
  precision: *single_double_precisions
  transA: [ N, T ]
  matrix_size: *medium_matrix_size_range
  incx: 2
  incy: 1
  alpha_beta: *alpha_beta_range
  threads_streams: *common_threads_streams
  devices: [0, 2, 4]

- name: gemv_medium_multi_threads_streams
  category: nightly
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *medium_matrix_size_range
  incx: 2
  incy: 1
  alpha_beta: *alpha_beta_range
  threads_streams: *common_threads_streams

- name: gemv_large
  category: nightly
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [  N, T, C ]
  matrix_size: *large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small

- name: gemv_very_large
  category: nightly
  function: gemv
  precision: *double_precision
  transA: [ N ]
  matrix_size: *very_large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small

- name: gemv_skinny_n
  category: pre_checkin
  function: gemv
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *skinny_n_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range

- name: gemv_size_t_non_transpose_90a
  category: stress
  function: gemv
  precision: *single_precision
  transA: [ N ]
  matrix_size: *size_t_non_transpose_90a
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  atomics_mode: [1]
  os_flags: LINUX
  gpu_arch: ['90a']

- name: gemv_size_t_non_transpose_908
  category: stress
  function: gemv
  precision: *single_precision
  transA: [ N ]
  matrix_size: *size_t_908
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['908']

- name: gemv_size_t_non_transpose_fp16_90a
  category: stress
  function: gemv_batched
  precision: *hpa_half_precision
  transA: [ N ]
  matrix_size: *size_t_fp16
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['90a']

- name: gemv_size_t_transpose_90a
  category: stress
  function: gemv
  precision: *single_precision
  transA: [ T, C ]
  matrix_size: *size_t_transpose_90a
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['90a']

- name: gemv_size_t_conj_90a
  category: stress
  function: gemv
  precision: *double_precision
  transA: [ T, C ]
  matrix_size: *size_t_general_c_90a
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['90a']

- name: gemv_size_t_transpose_908
  category: stress
  function: gemv
  precision: *single_precision
  transA: [ T, C ]
  matrix_size: *size_t_db_loads_t_908
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['908']

# No CI running this currently. Initially tested locally.
- name: gemv_size_t_transpose_10
  category: stress
  function: gemv
  precision: *single_precision
  transA: [ T, C ]
  matrix_size: *size_t_gfx10_wr
  alpha_beta: *alpha_beta_range_size_t
  pointer_mode_host: false
  os_flags: LINUX
  gpu_arch: ['11**', '10**']

# gemv_batched
- name: gemv_batched_bad_arg
  category: pre_checkin
  function: gemv_batched_bad_arg
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: N
  api: [ C, FORTRAN ]

- name: gemv_batched_arg_check
  category: quick
  function: gemv_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: N

- name: gemv_batched_NaN
  category: pre_checkin
  function: gemv_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *all_algo_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha: [ 1.0, 2.0, .NaN ] # NaN is converted to 0.0 in test code
  beta: [2.0, 1.0, .NaN ]
  batch_count: [ 2, 9 ] # 9 for qmc_pack algo

- name: gemv_batched_fortran
  category: quick
  function: gemv_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ -1, 0, 3 ]
  api: FORTRAN

- name: gemv_batched_double_buffered
  category: pre_checkin
  function: gemv_batched
  precision: *single_double_precisions
  transA: [ N ]
  matrix_size: *double_buffered_loads_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 1, 3 ]
  atomics_mode: [0, 1]

- name: gemv_batched_skinny_n
  category: pre_checkin
  function: gemv_batched
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *skinny_n_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]

- name: gemv_batched_small
  category: quick
  function: gemv_batched
  precision: *single_double_precisions
  transA: [ N, T, C ]
  matrix_size: *small_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemv_sm_mn_batched
  category: pre_checkin
  function:
    - gemv_batched
    - gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N ]
  matrix_size: *sm_mn_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha: [ 1.0, .NaN ]  # NaN is converted to 0.0 in test code
  beta: [ 1.0, .NaN ]
  batch_count: [256, 513] # >= 256

- name: gemv_batched_qmcpack
  category: quick
  function: gemv_batched
  precision: *single_double_precisions_complex_real
  transA: [ T, C ]
  matrix_size: *qmcpack_matrix_size_range
  incx: [ 2 ]
  incy: [ 2 ]
  alpha_beta: *alpha_beta_range
  batch_count: [ 100, 1000 ]

- name: gemv_batched_medium
  category: pre_checkin
  function: gemv_batched
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  matrix_size: *medium_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range
  batch_count: [ 3 ]

- name: gemv_batched_large
  category: nightly
  function: gemv_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]

- name: gemv_batched_very_large
  category: nightly
  function: gemv_batched
  precision: *double_precision
  transA: [ N ]
  matrix_size: *very_large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]

# gemv_strided_batched
- name: gemv_strided_batched_bad_arg
  category: pre_checkin
  function: gemv_strided_batched_bad_arg
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: N
  api: [ C, FORTRAN ]

- name: gemv_strided_batched_arg_check
  category: quick
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: N

- name: gemv_strided_batched_NaN
  category: pre_checkin
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *all_algo_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha: [ 2.0, .NaN ] # NaN converted to 0.0 in test code
  beta:  [ 0.5, 1.0, .NaN ]
  batch_count: [ 2 ]

- name: gemv_strided_batched_fortran
  category: quick
  function: gemv_strided_batched
  precision: *single_double_precisions_complex_real
  transA: [ N, T, C ]
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ -1, 0, 3 ]
  api: FORTRAN

- name: gemv_strided_batched_small
  category: quick
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *small_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range
  batch_count: [ -1, 0, 1, 3 ]

- name: gemv_strided_batched_double_buffered
  category: pre_checkin
  function: gemv_strided_batched
  precision: *single_double_precisions
  transA: [ N ]
  matrix_size: *double_buffered_loads_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 1, 3 ]
  atomics_mode: [0, 1]

- name: gemv_strided_batched_skinny_n
  category: pre_checkin
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *skinny_n_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]


- name: gemv_strided_batched_qmcpack
  category: quick
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ T, C ]
  matrix_size: *qmcpack_matrix_size_range
  incx: [ 2 ]
  incy: [ 2 ]
  stride_scale: 1
  alpha_beta: *alpha_beta_range
  batch_count: [ 64, 512 ]

- name: gemv_strided_batched_medium
  category: pre_checkin
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N, T, C ]
  matrix_size: *medium_matrix_size_range
  incx_incy: *incx_incy_range
  alpha_beta: *alpha_beta_range
  batch_count: [ 3 ]

- name: gemv_strided_batched_large
  category: nightly
  function: gemv_strided_batched
  precision: *single_double_precisions
  transA: [ N, T, C ]
  matrix_size: *large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]

- name: gemv_strided_batched_very_large
  category: nightly
  function: gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: [ N ]
  matrix_size: *very_large_matrix_size_range
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 3 ]

- name: gemv_graph_test
  category: pre_checkin
  function:
        - gemv
        - gemv_batched
        - gemv_strided_batched
  precision: *gemv_bfloat_half_single_double_complex_real_precisions
  transA: N
  matrix_size:
        - { M:   100, N:   200, lda:  200, stride_a:    40000 }
  incx_incy: *incx_incy_range_small
  alpha_beta: *alpha_beta_range_small
  batch_count: [ 5 ]
  graph_test: true
...
